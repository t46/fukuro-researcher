{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141c69876dcc476993031d2c71c33289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512e7b2ed2ca496db887bd990888d9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecfd4f62b444ce785c5686213b3f084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/fukuro-researcher/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "   The [`BertModel`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary.\n",
      "\n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "\n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
      "            1]`:\n",
      "\n",
      "            - 0 corresponds to a *sentence A* token,\n",
      "            - 1 corresponds to a *sentence B* token.\n",
      "\n",
      "            [What are token type IDs?](../glossary#token-type-ids)\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.max_position_embeddings - 1]`.\n",
      "\n",
      "            [What are position IDs?](../glossary#position-ids)\n",
      "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
      "            the model is configured as a decoder.\n",
      "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
      "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        \n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`BertConfig`]) and inputs.\n",
      "\n",
      "        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the model.\n",
      "        - **pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) -- Last layer hidden-state of the first token of the sequence (classification token) after further processing\n",
      "          through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n",
      "          the classification token after processing through a linear layer and a tanh activation function. The linear\n",
      "          layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      "          weighted average in the cross-attention heads.\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "          encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
      "          `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n",
      "          input) to speed up sequential decoding.\n",
      "  \n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import AutoTokenizer, BertModel\n",
      "    >>> import torch\n",
      "\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      "    >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      "\n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    >>> outputs = model(**inputs)\n",
      "\n",
      "    >>> last_hidden_states = outputs.last_hidden_state\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# モデルとトークナイザーをロード\n",
    "model_name = \"bert-base-uncased\"  # 例としてBERTを使用\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# サンプルテキストをエンコード\n",
    "text = \"Hello, world!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# モデルの期待する入力形式を確認\n",
    "print(inputs)\n",
    "\n",
    "# モデルのforwardメソッドのシグネチャを確認\n",
    "print(model.forward.__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
