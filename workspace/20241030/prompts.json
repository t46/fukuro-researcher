{"dataset": "\n    Research Context: \n    The hallucination problem in Large Language Models (LLMs) \u2014 where models generate inaccurate or non-factual content \u2014 is a critical issue, especially in high-stakes fields like healthcare, law, and education. \n\n    Challenge: Automatic Detection and Evaluation of Hallucinations\n\n    Background: Hallucinated content lacks clear error messages, making manual verification costly and time-consuming.\n    \n    Proposition Idea: \n    - Consistency Checking Algorithm: Generate multiple outputs and detect inconsistencies among them.\n    - Fact-checking with Ensemble Models: Integrate multiple fact-checking engines to cross-validate outputs in real time.\n    - Context-aware Error Detection: Automatically identify hallucinations by comparing generated text with pre-defined knowledge bases.\n    \n\n    Your final goal is to implement a Proposition Idea and design experiments to validate its feasibility based on the Research Context. To achieve this, generate search query for obtaining datasets from HuggingFace using an LLM. This prompt should explicitly state the purpose and objectives of the research, what the Proposition Idea is, and which datasets should be retrieved to validate it.\n\n    query will be used as follows:\n    from huggingface_hub import HfApi\n    api = HfApi()\n    results = list(api.list_datasets(search=query, sort=\"downloads\", direction=-1, limit=max_results))\n\n    Note that query input to `search` is a string that will be contained in the returned datasets.\n\n    Generate only one query and the query should be single word.\n    For example, query will be a dataset name like \"wikitext\", \"cifar10\", \"imagenet\", \"sarcasm\", \"emotion\", etc.\n\n    <query>\n    \"...\"\n    </query>\n    ", "model": "\n    Research Context: \n    The hallucination problem in Large Language Models (LLMs) \u2014 where models generate inaccurate or non-factual content \u2014 is a critical issue, especially in high-stakes fields like healthcare, law, and education. \n\n    Challenge: Automatic Detection and Evaluation of Hallucinations\n\n    Background: Hallucinated content lacks clear error messages, making manual verification costly and time-consuming.\n    \n    Proposition Idea: \n    - Consistency Checking Algorithm: Generate multiple outputs and detect inconsistencies among them.\n    - Fact-checking with Ensemble Models: Integrate multiple fact-checking engines to cross-validate outputs in real time.\n    - Context-aware Error Detection: Automatically identify hallucinations by comparing generated text with pre-defined knowledge bases.\n    \n\n    Your final goal is to implement a Proposition Idea and design experiments to validate its feasibility based on the Research Context. To achieve this, generate search queryfor obtaining baseline models from HuggingFace using an LLM. This prompt should explicitly state the purpose and objectives of the research, what the Proposition Idea is, and which models should be retrieved to validate it.\n\n    query will be used as follows:\n    from huggingface_hub import HfApi\n    api = HfApi()\n    results = list(api.list_models(search=query, sort=\"downloads\", direction=-1))\n\n    Note that query input to `search` is a string that will be contained in the returned models.\n\n    Generate only one query and the query should be single word.\n    For example, query can will a model name like \"resnet\", \"lstm\", \"bert\", \"gpt2\", \"t5\", etc.\n\n    <query>\n    \"...\"\n    </query>\n    "}